# Курсовая работа
## Активное обучение

В данной работе реализованы и проверены следующие алгоритмы активного обучения:
* Least Confidence (LC)
* Coreset
* Maximum Normalized Log-Probability (MNLP)
* BADGE (Batch Active learning by Diverse Gradient Embedding)
* BAIT (Batch Active learning via Information maTrice)
* BALD (Bayesian Active Learning by Disagreement)

Также было произведено обучение модели без на всех данных и на части выборки без использования активного обучения.

Для работы был выбран датасет CIFAR10. В качестве базовой модели взяли предобученный EfficientNet-b0.

Описание алгоритмов и их реализация приведена в ноутбуке

### Результаты

По оси Х указан изначальный объем даных (доля), который был размечен.  
По оси Y указан скор на тестовом датасете.  
Каждая линия соответствует какому-либо алгоритму активного обучения.  
Также присутствуют метрики модели, обученной на всем наборе данных (full_data) и метрики модели, обученной на части данных без активного обучения (no_active_learn).

![](result.png)

* использование алгоритмов активного обучение показало сильный прирост по качеству в сравнении с моделью, просто обученной на части выборки
* при этом качество модели на всем датасете все же выше
* однако мы видим размен - можно использовать в ~5 раз меньше данных при обучении и потерять ~7 процентов скора. Для некоторых бизнес-задач такой размен может быть приемлемым, учитывая дороговизну разметки данных
* все алгоритмы активного обучения отработали примерно одинаково. Coreset selection оказался чуть лучше всех остальных, но при этом он работает сильно медленнее
* с точки зрения баланса скорости и качества стоит использовать, например, алгоритмы BALD, BAIT, MNLP
